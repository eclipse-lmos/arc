"use strict";(self.webpackChunkarc=self.webpackChunkarc||[]).push([[1831],{8490:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>i,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>n,toc:()=>c});var n=t(8916),l=t(4848),r=t(8453);const o={title:"Llama3 is out!"},s=void 0,i={authorsImageUrls:[]},c=[];function m(e){const a={a:"a",code:"code",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsxs)(a.p,{children:["Meta have released a new version of their large language model, Llama3, ",(0,l.jsx)(a.a,{href:"https://ai.meta.com/blog/meta-llama-3/",children:"https://ai.meta.com/blog/meta-llama-3/"}),"."]}),"\n",(0,l.jsxs)(a.p,{children:["Thanks to ollama, the model can easily be tested locally, see ",(0,l.jsx)(a.a,{href:"https://ollama.com/library/llama3",children:"https://ollama.com/library/llama3"})]}),"\n",(0,l.jsxs)(a.p,{children:["Simply pull the model, run ",(0,l.jsx)(a.code,{children:"ollama"})," and start using it with Arc!"]}),"\n",(0,l.jsx)(a.pre,{children:(0,l.jsx)(a.code,{className:"language-bash",children:"ollama pull llama3:8b\nollama serve\n"})}),"\n",(0,l.jsx)(a.p,{children:'Now llama3:8b is ready to use with Arc and the ollama client.\nWe recommend using the 8B variant locally, as it "only" requires 8GB of RAM.'})]})}function d(e={}){const{wrapper:a}={...(0,r.R)(),...e.components};return a?(0,l.jsx)(a,{...e,children:(0,l.jsx)(m,{...e})}):m(e)}},8453:(e,a,t)=>{t.d(a,{R:()=>o,x:()=>s});var n=t(6540);const l={},r=n.createContext(l);function o(e){const a=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),n.createElement(r.Provider,{value:a},e.children)}},8916:e=>{e.exports=JSON.parse('{"permalink":"/arc/blog/Llama3","source":"@site/blog/Llama3.md","title":"Llama3 is out!","description":"Meta have released a new version of their large language model, Llama3, https://ai.meta.com/blog/meta-llama-3/.","date":"2024-11-15T08:57:22.000Z","tags":[],"readingTime":0.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Llama3 is out!"},"unlisted":false,"nextItem":{"title":"Sptring.ai integration using the SpringChatClient","permalink":"/arc/blog/Spring.AI-integration"}}')}}]);